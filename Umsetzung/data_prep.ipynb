{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "#file references\n",
    "data_dump = \"data/data_dump.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# read JSON file and save data into list of dicts\n",
    "############################################################\n",
    "\n",
    "json_file = open(data_dump)\n",
    "json_str = json_file.read()\n",
    "json_data = json.loads(json_str) # is list of dicts\n",
    "\n",
    "# nbr of elements in data_dump \n",
    "print(len(json_data)) \n",
    "# data_dict = (json_data)[1] #dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# reducing data size (making a copy of json_data)\n",
    "# removes meta tags\n",
    "# removes description -> tree\n",
    "# removes entity -> ID\n",
    "# removes entity -> value -> thesaurus entries\n",
    "# removes entity -> rect -> coordinates \n",
    "############################################################\n",
    "\n",
    "# keys that should remain in the dicts --> desc. (contains entities, relations & tree) \n",
    "keep = [\"description\"] # [\"archivnr\",\"material\",\"kuenstler\",\"bildthema\",\"description\"]  \n",
    "                     \n",
    "\n",
    "skinny_data = []                       \n",
    "for diction in json_data:               \n",
    "    skinny_dict = {k: diction[k] for k in keep if k in diction}\n",
    "    #skinny_data.append(skinny_dict)\n",
    "\n",
    "\n",
    "    print('removed: ',skinny_dict['description'].pop('tree', None))\n",
    "    \n",
    "    \n",
    "    entities = skinny_dict['description']['entities']\n",
    "    for x in enumerate(entities): #x gives index int\n",
    "        \n",
    "        index = ''\n",
    "        #print('----')\n",
    "        #print ('Orig. ',x[1]) #x[1] is the dict\n",
    "        #print('vals ',x[1].values())\n",
    "        #print('-')\n",
    "        #print('removed: ', x[1].pop('id',None))\n",
    "        x[1].pop('id',None)\n",
    "        for y in enumerate(x[1].items()):\n",
    "            temp_val = ''\n",
    "            temp_key = ''\n",
    "            #print(y[1])\n",
    "            #print('+')\n",
    "            if type(y[1][1]) == list:\n",
    "                if type(y[1][1][0]) == dict:\n",
    "                    temp_key = y[1][0]\n",
    "                    #print('val: ',y[1][1][0]['value'])\n",
    "                    if 'value' in y[1][1][0]:\n",
    "                        temp_val = y[1][1][0]['value']\n",
    "                        x[1][temp_key] = temp_val\n",
    "                    else: \n",
    "                        print('no value key in: ', y[1][1][0])\n",
    "                    if 'coordinates' in y[1][1][0]:\n",
    "                        #print('removed: ', x[1].pop('coordinates',None))\n",
    "                        x[1].pop('coordinates',None)\n",
    "                \n",
    "            #if type(y[1][1]) == list:\n",
    "                #print('is list: ', y[1][1])\n",
    "            #x[1][temp_key]\n",
    "            #print(sorted(x[1].items()))\n",
    "\n",
    "\n",
    "\n",
    "    skinny_data.append(skinny_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# save list of dicts into JSON file\n",
    "############################################################\n",
    "\n",
    "# replaces special characters\n",
    "# ä->ae | ö->oe | ü->ue | ß->sz\n",
    "\n",
    "jsonList = json.dumps(skinny_data).replace(\"\\\\u00c3\\\\u00a4\",\"ae\").replace(\"\\\\u00c3\\\\u00bc\",\"ue\").replace(\"\\\\u00c3\\\\u0178\",\"sz\").replace(\"\\\\u00c3\\\\u201e\",\"oe\").replace(\"\\\\u00c3\\\\u00b6\",\"oe\").replace(\"\\\\u00c3\\\\u2013\",\"oe\").replace(\"\\\\u00c3\\\\u0153\",\"ue\").replace(\"\\\\u00c3\\\\u00a9\",\"\")\n",
    "\n",
    "\n",
    "jsonFile = open(\"data/skinny_data.json\", \"w\")\n",
    "jsonFile.write(jsonList)\n",
    "jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# creating more dicts and saving into json\n",
    "# Making Dataframes (columns: from(entity) - to(entity) - relation)\n",
    "# Part 1\n",
    "############################################################\n",
    "data_processed = \"data/skinny_data.json\"\n",
    "json_file = open(data_processed)\n",
    "json_str = json_file.read()\n",
    "json_data = json.loads(json_str) # is list of dicts\n",
    "\n",
    "skinny_data = json_data\n",
    "#new dicts that only contain relations/entities (easier to grab data)\n",
    "relation_dict = []\n",
    "entity_dict = []\n",
    "for diction in skinny_data:\n",
    "    for key, value in diction.items():\n",
    "        for k, v in value.items():\n",
    "            if k == 'entities':\n",
    "                #print(v)\n",
    "                #for x in v:\n",
    "                    #print(x)\n",
    "                entity_dict.append(v)\n",
    "            if k == 'relations':\n",
    "                #print(v)\n",
    "                relation_dict.append(v)\n",
    "\n",
    "entityList = json.dumps(entity_dict)\n",
    "entityFile = open(\"data/entity_data.json\", \"w\")\n",
    "entityFile.write(entityList)\n",
    "entityFile.close()\n",
    "\n",
    "relList = json.dumps(relation_dict)\n",
    "relFile = open(\"data/relation_data.json\", \"w\")\n",
    "relFile.write(relList)\n",
    "relFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Making Dataframes (columns: from(entity) - to(entity) - relation)\n",
    "# Part 2\n",
    "############################################################\n",
    "entity_dict = json.loads(open(\"data/entity_data.json\").read()) # load entity dictionaries\n",
    "relation_dict = json.loads(open(\"data/relation_data.json\").read()) # iload relation dictionaries\n",
    "\n",
    "#### adding the archive number\n",
    "archiv_list = []\n",
    "for diction in json_data:\n",
    "    archivnr = diction['archivnr']\n",
    "    archiv_list.append(archivnr)\n",
    "####\n",
    "\n",
    "index = 0\n",
    "df_list=[] #list of multiple KGs\n",
    "list_of_lists = [] #list of KGs as lists rather than DFs\n",
    "for diction in relation_dict:\n",
    "    KnGr=[] # knowledge graph of image as a list\n",
    "    KnGr.append([\"Archivnummer\",str(archiv_list[index]),\"hat ID\"])\n",
    "    for value in diction:\n",
    "        entity_from = entity_dict[index][value[0]] \n",
    "        entity_to = entity_dict[index][value[1]]\n",
    "        relation = value[2]\n",
    "        \n",
    "        #print(type(entity_from))\n",
    "        #print(entity_from)\n",
    "        #temp_entity = entity(entity_from)\n",
    "        #print(type(temp_entity.val))\n",
    "        KnGr.append([str(entity_from),str(entity_to),relation])\n",
    "        #print(KnGr[0])\n",
    "        \n",
    "    #print(KnGr)\n",
    "    \n",
    "    list_of_lists.append(KnGr)\n",
    "    df=pd.DataFrame(KnGr,columns=['from','to','rel'])\n",
    "    df.style\n",
    "    df_list.append(df) #KnGr_list\n",
    "    index +=1\n",
    "#print(KnGr_list)\n",
    "\n",
    "\n",
    "pickle.dump(df_list, open('data/all_Dataframes.pkl', 'wb')) # save all dataframes stored in a list\n",
    "pickle.dump(list_of_lists, open('data/all_KGs_as_lists.pkl', 'wb')) # save all KGs in list-form stored in list\n",
    "pickle.dump(archiv_list, open('data/archivlist.pkl','wb')) # just for double checking, the archive numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dict = json.loads(open(\"data/entity_data.json\").read()) # load entity dictionaries\n",
    "relation_dict = json.loads(open(\"data/relation_data.json\").read()) # iload relation dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Making Dataframes (columns: from(entity) - to(entity) - relation)\n",
    "# Part 3 - altering the graphs to add more entities\n",
    "############################################################\n",
    "\n",
    "\n",
    "\n",
    "index = 0\n",
    "all_dfs = [] # all dataframes as a list\n",
    "#####\n",
    "df = []\n",
    "all_e_r_lists =[]\n",
    "#####\n",
    "\n",
    "for diction in relation_dict:\n",
    "    \n",
    "    ####\n",
    "    dfs=[] # list of dataframes, not converted\n",
    "    ####\n",
    "    e_r_list = []\n",
    "    e_r_list.append([\"Archivnummer\",str(archiv_list[index]),\"hat ID\"])\n",
    "    for value in diction:\n",
    "        entity_from = entity_dict[index][value[0]] \n",
    "        entity_to = entity_dict[index][value[1]]\n",
    "        relation = value[2]\n",
    "        ######################\n",
    "        # this is where the entites should be split\n",
    "        ######################\n",
    "        \n",
    "        e_r = [entity_from,entity_to,relation]\n",
    "        e_r_list.append(e_r)\n",
    "\n",
    "        #print(entity_from)\n",
    "        if len(entity_from)>1:\n",
    "            for i,(k,v) in enumerate(entity_from.items()):\n",
    "                #print(i, v)\n",
    "                e_r = [entity_from,v,k]\n",
    "                #print(e_r)\n",
    "                e_r_list.append(e_r)\n",
    "\n",
    "        ####\n",
    "    #print(e_r_list)   \n",
    "    all_e_r_lists.append(e_r_list) \n",
    "    #print(all_e_r_lists)\n",
    "    df = pd.DataFrame(e_r_list,columns=['from','to','rel'])  \n",
    "    all_dfs.append(df) \n",
    "    #print(KnGr)\n",
    "    #list_of_lists.append(KnGr)\n",
    "    #all_dfs.append(dfs)\n",
    "    #df=pd.DataFrame(KnGr,columns=['from','to','rel'])\n",
    "    #df.style\n",
    "    #df_list.append(df) #KnGr_list\n",
    "    index +=1\n",
    "#print(KnGr_list)\n",
    "\n",
    "\n",
    "pickle.dump(all_dfs, open('data/all_Dataframes2.pkl', 'wb')) # save all dataframes stored in a list\n",
    "pickle.dump(all_e_r_lists, open('data/all_KGs_as_lists2.pkl', 'wb')) # save all KGs in list-form stored in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# removing the keys (just makes it easier to read)\n",
    "# converting df entries to string\n",
    "############################################################\n",
    "#load the data\n",
    "all_dfs = pickle.load(open('data/all_Dataframes2.pkl', 'rb'))\n",
    "all_e_r_lists = pickle.load(open('data/all_KGs_as_lists2.pkl','rb'))\n",
    "\n",
    "one_df = all_dfs[0]\n",
    "\n",
    "one_df.style #testing that the df is correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(one_df,open('graphs/random/sample_df_altered.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dfs=[]\n",
    "all_rows= []\n",
    "\n",
    "for df in all_dfs:\n",
    "    rows=[]\n",
    "    df_list = df.values.tolist()\n",
    "\n",
    "    for val in df_list:\n",
    "        #print(val)\n",
    "        row = []\n",
    "        \n",
    "        for v in val:\n",
    "            #print(v)\n",
    "            if type(v)==dict: \n",
    "                my_list = []\n",
    "                for x in v.values():\n",
    "                    #print('x ', x)\n",
    "                    my_list.append(x)\n",
    "                    i = '_'.join(map(str, my_list))\n",
    "                row.append(str(i))\n",
    "            else:\n",
    "                row.append(str(v))\n",
    "        #print('list ',row)\n",
    "        rows.append(row)\n",
    "    new_df = pd.DataFrame(rows,columns=['from','to','rel']) \n",
    "    new_dfs.append(new_df)\n",
    "    all_rows.append(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(new_dfs, open('data/all_Dataframes2.pkl', 'wb'))\n",
    "pickle.dump(all_rows, open('data/all_rows.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_dfs))\n",
    "\n",
    "new_dfs[5].style"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7927a50cdc078fee6c0386384f02243f9c31fc413ad7647fe6d6c063eb6496c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
