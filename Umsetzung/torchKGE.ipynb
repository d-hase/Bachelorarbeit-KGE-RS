{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge Graph Embedding, model training, and evaluation using the TorchKGE library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# imports\n",
    "############################################################\n",
    "\n",
    "from torch import cuda\n",
    "from torch.optim import Adam\n",
    "import pickle \n",
    "\n",
    "from torchkge.models import TransEModel, ComplExModel, ConvKBModel\n",
    "from torchkge.sampling import BernoulliNegativeSampler, UniformNegativeSampler\n",
    "from torchkge.utils import Trainer, MarginLoss, DataLoader\n",
    "from torchkge.evaluation import LinkPredictionEvaluator\n",
    "from torchkge.evaluation import RelationPredictionEvaluator\n",
    "from torchkge.evaluation import TripletClassificationEvaluator\n",
    "\n",
    "import json\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torchkge.data_structures import KnowledgeGraph\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# load files\n",
    "############################################################\n",
    "\n",
    "#list_of_lists = pickle.load(open('data/all_KGs_as_lists2.pkl', 'rb'))\n",
    "list_of_lists = pickle.load(open('data/all_rows.pkl', 'rb'))\n",
    "#df_list\n",
    "df_list_old = pickle.load(open('data/all_Dataframes.pkl','rb'))\n",
    "df_list = pickle.load(open('data/all_Dataframes2.pkl', 'rb'))\n",
    "#all Knowledge Graph objects\n",
    "KGs_old = pickle.load(open('data/all_Knowledge_Graphs.pkl','rb'))\n",
    "KGs = pickle.load(open('data/all_Knowledge_Graphs2.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8471\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# 1. making KG objects from dataframes\n",
    "# note: no need to run again unless changes were made to 'list_of_lists', just load 'KGs' instead\n",
    "############################################################\n",
    "KGs = [] \n",
    "for row in list_of_lists:\n",
    "    if row:\n",
    "        data=pd.DataFrame(row,columns=['from','to','rel'])\n",
    "        kg = KnowledgeGraph(df=data)\n",
    "        KGs.append(kg)\n",
    "\n",
    "print(len(KGs)) # total number of dataframes: 8471 (8469)\n",
    "pickle.dump(KGs, open('data/all_Knowledge_Graphs2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# collect KGs with 'Handlung: Kreuzigen'\n",
    "# idea: mimics what a user has previously looked at & uses that as base for recommendation\n",
    "############################################################\n",
    "\n",
    "KGs = pickle.load(open('data/all_Knowledge_Graphs2.pkl', 'rb'))\n",
    "graphs = []\n",
    "sets = []\n",
    "for x in enumerate(KGs):\n",
    "    index = x[0]\n",
    "    kg = x[1]\n",
    "    df = kg.get_df()\n",
    "    val = df['to'].str.contains('kreuzigen').sum()\n",
    "    if val > 0:\n",
    "        graphs.append(df)\n",
    "        #print('kreuzigen was found in: ',index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# preparing kg_train, kg_val for model and eval\n",
    "############################################################\n",
    "print(len(graphs))\n",
    "complete_KG = pd.concat(graphs[:5], axis=0)\n",
    "#complete_KG = graphs[:2]\n",
    "complete_KG = KnowledgeGraph(df=complete_KG)\n",
    "sizes = complete_KG.get_sizes(complete_KG.n_facts,share=0.8)\n",
    "sets = complete_KG.split_kg(share=0.8,sizes=sizes,validation=True)\n",
    "kg_train = sets[0]\n",
    "kg_val = sets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datafr = complete_KG.get_df()\n",
    "\n",
    "pickle.dump(datafr, open('results/data/model_input_df.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\debor\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "Epoch 1000 | mean loss: 1.62740: 100%|██████████| 1000/1000 [00:54<00:00, 18.31epoch/s]\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# Model Training TransE\n",
    "############################################################\n",
    "#KGs = pickle.load(open('data/all_Knowledge_Graphs.pkl', 'rb'))\n",
    "\n",
    "# Define some hyper-parameters for training\n",
    "emb_dim = 10\n",
    "lr = 0.0004\n",
    "n_epochs = 1000\n",
    "b_size = 128\n",
    "margin = 0.5\n",
    "\n",
    "kg_train = complete_KG\n",
    "\n",
    "# Define the model and criterion\n",
    "model = TransEModel(emb_dim, kg_train.n_ent, kg_train.n_rel, dissimilarity_type='L2')\n",
    "criterion = MarginLoss(margin)\n",
    "\n",
    "# Move everything to CUDA if available\n",
    "if cuda.is_available():\n",
    "    cuda.empty_cache()\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "\n",
    "# Define the torch optimizer to be used\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "sampler = BernoulliNegativeSampler(kg_train) #UniformNegativeSampler(kg_train, kg_val=kg_val, kg_test=kg_test)\n",
    "dataloader = DataLoader(kg_train, batch_size=b_size)\n",
    "\n",
    "iterator = tqdm(range(n_epochs), unit='epoch')\n",
    "for epoch in iterator:\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        h, t, r = batch[0], batch[1], batch[2]\n",
    "        n_h, n_t = sampler.corrupt_batch(h, t, r)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        pos, neg = model(h, t, r, n_h, n_t)\n",
    "        loss = criterion(pos, neg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    iterator.set_description(\n",
    "        'Epoch {} | mean loss: {:.5f}'.format(epoch + 1,\n",
    "                                              running_loss / len(dataloader)))\n",
    "    model.normalize_parameters()\n",
    "model.normalize_parameters()\n",
    "\n",
    "pickle.dump(model, open('results/data/model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Count:  27 / 8471\n",
      "8444\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Triplet classification evaluation on test set by learning thresholds on validation set\n",
    "model = pickle.load(open('results/deep/model.pkl', 'rb')) #change file if needed\n",
    "KGs = pickle.load(open('data/all_Knowledge_Graphs2.pkl', 'rb'))\n",
    "err_count = 0\n",
    "eval_results = []\n",
    "for x in enumerate(KGs):\n",
    "    #print (x[0])\n",
    "    index = x[0]\n",
    "    if type(x[1]) == KnowledgeGraph:\n",
    "        try:\n",
    "            kg_test = x[1]\n",
    "\n",
    "            evaluator = TripletClassificationEvaluator(model, kg_val, kg_test)\n",
    "            evaluator.evaluate(b_size=264)\n",
    "            eval_res = evaluator.accuracy(b_size=264)\n",
    "            eval_results.append([eval_res, kg_test])\n",
    "            #print(eval_res)\n",
    "            #print('Accuracy on test set: {}'.format(evaluator.accuracy(b_size=128)))\n",
    "\n",
    "            #evaluator = LinkPredictionEvaluator(model,kg_test)\n",
    "            #evaluator.evaluate(b_size=264)\n",
    "            #evaluator.print_results(eval_results)\n",
    "        except:\n",
    "            err_count+=1\n",
    "    \n",
    "print(\"Error Count: \",err_count,\"/\",len(KGs))    \n",
    "print(len(eval_results))\n",
    "\n",
    "pickle.dump(eval_results, open(\"results/deep/Triplet_Classification.pkl\", \"wb\")) #change file if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(eval_results, open(\"results/transE/Triplet_Classification.pkl\", \"wb\"))\n",
    "pickle.dump(model, open(\"results/transE/model.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to make the different evals#\n",
    "#eval = RelationPredictionEvaluator(model, KGs[9], directed=True)\n",
    "#eval.evaluate(b_size=b_size)\n",
    "#eval.print_results()\n",
    "\n",
    "# Link prediction evaluation on test set.\n",
    "#evaluator = LinkPredictionEvaluator(model, KGs[9])\n",
    "#evaluator.evaluate(b_size=32)\n",
    "#evaluator.print_results()\n",
    "\n",
    "# Triplet classification evaluation on test set by learning thresholds on validation set\n",
    "#evaluator = TripletClassificationEvaluator(model, kg_val, kg_test)\n",
    "#evaluator.evaluate(b_size=128)\n",
    "#print('Accuracy on test set: {}'.format(evaluator.accuracy(b_size=128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Count:  27 / 8471\n",
      "8444\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# relation prediction evaluation on test set\n",
    "model = pickle.load(open('results/bilinear/model.pkl', 'rb'))\n",
    "KGs = pickle.load(open('data/all_Knowledge_Graphs2.pkl', 'rb'))\n",
    "err_count=0\n",
    "eval_results = []\n",
    "for x in enumerate(KGs):\n",
    "    #print (x[0])\n",
    "    index = x[0]\n",
    "    if type(x[1]) == KnowledgeGraph:\n",
    "        try:\n",
    "            kg_test = x[1]\n",
    "            eval = RelationPredictionEvaluator(model, kg_test, directed=True)\n",
    "            eval.evaluate(b_size=264, verbose=False)\n",
    "            #eval.print_results()\n",
    "            mean = eval.mean_rank()\n",
    "            hit = eval.hit_at_k()\n",
    "            mrr = eval.mrr()\n",
    "            eval_results.append([[mean,hit,mrr], kg_test])\n",
    "            #print(eval_res)\n",
    "            #print('Accuracy on test set: {}'.format(evaluator.accuracy(b_size=128)))\n",
    "\n",
    "            #evaluator = LinkPredictionEvaluator(model,kg_test)\n",
    "            #evaluator.evaluate(b_size=264)\n",
    "            #eval.print_results()\n",
    "        except:\n",
    "            err_count+=1\n",
    "    \n",
    "print(\"Error Count: \",err_count,\"/\",len(KGs))    \n",
    "print(len(eval_results))\n",
    "\n",
    "pickle.dump(eval_results, open(\"results/bilinear/Relation_Prediction.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(eval_results, open(\"graphs/first_try/eval_TransE_Rel_Predict_2.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Count:  27 / 8471\n",
      "8444\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# link prediction evaluation on test set\n",
    "model = pickle.load(open('results/bilinear/model.pkl', 'rb'))\n",
    "KGs = pickle.load(open('data/all_Knowledge_Graphs2.pkl', 'rb'))\n",
    "err_count=0\n",
    "eval_results = []\n",
    "for x in enumerate(KGs):\n",
    "    #print (x[0])\n",
    "    index = x[0]\n",
    "    if type(x[1]) == KnowledgeGraph:\n",
    "        try:\n",
    "            kg_test = x[1]\n",
    "            eval = LinkPredictionEvaluator(model, kg_test)\n",
    "            eval.evaluate(b_size=264, verbose=False)\n",
    "            #eval.print_results()\n",
    "            mean = eval.mean_rank()\n",
    "            hit = eval.hit_at_k()\n",
    "            mrr = eval.mrr()\n",
    "            eval_results.append([[mean,hit,mrr], kg_test])\n",
    "            #print(eval_res)\n",
    "            #print('Accuracy on test set: {}'.format(evaluator.accuracy(b_size=128)))\n",
    "\n",
    "            #evaluator = LinkPredictionEvaluator(model,kg_test)\n",
    "            #evaluator.evaluate(b_size=264)\n",
    "            #eval.print_results()\n",
    "        except:\n",
    "            err_count+=1\n",
    "    \n",
    "print(\"Error Count: \",err_count,\"/\",len(KGs))    \n",
    "print(len(eval_results))\n",
    "\n",
    "pickle.dump(eval_results, open(\"results/bilinear/Link_Prediction.pkl\", \"wb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bilinear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\debor\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "Epoch 1000 | mean loss: 0.47335: 100%|██████████| 1000/1000 [00:54<00:00, 18.26epoch/s]\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# Model Training ComplEx\n",
    "############################################################\n",
    "#KGs = pickle.load(open('data/all_Knowledge_Graphs.pkl', 'rb'))\n",
    "# Load dataset\n",
    "#kg_train, _, _ = load_fb15k()\n",
    "#kg_train = KGs[:10]\n",
    "#kg_val = KGs[103]\n",
    "#kg_test = KGs[200]\n",
    "\n",
    "# Define some hyper-parameters for training\n",
    "emb_dim = 10\n",
    "lr = 0.0004\n",
    "n_epochs = 1000\n",
    "b_size = 128\n",
    "margin = 0.5\n",
    "\n",
    "kg_train = complete_KG\n",
    "\n",
    "# Define the model and criterion\n",
    "model = ComplExModel(emb_dim, kg_train.n_ent, kg_train.n_rel)\n",
    "criterion = MarginLoss(margin)\n",
    "\n",
    "# Move everything to CUDA if available\n",
    "if cuda.is_available():\n",
    "    cuda.empty_cache()\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "\n",
    "# Define the torch optimizer to be used\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "sampler = BernoulliNegativeSampler(kg_train) #UniformNegativeSampler(kg_train, kg_val=kg_val, kg_test=kg_test)\n",
    "dataloader = DataLoader(kg_train, batch_size=b_size)\n",
    "\n",
    "iterator = tqdm(range(n_epochs), unit='epoch')\n",
    "for epoch in iterator:\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        h, t, r = batch[0], batch[1], batch[2]\n",
    "        n_h, n_t = sampler.corrupt_batch(h, t, r)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        pos, neg = model(h, t, r, n_h, n_t)\n",
    "        loss = criterion(pos, neg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    iterator.set_description(\n",
    "        'Epoch {} | mean loss: {:.5f}'.format(epoch + 1,\n",
    "                                              running_loss / len(dataloader)))\n",
    "    model.normalize_parameters()\n",
    "model.normalize_parameters()\n",
    "\n",
    "pickle.dump(model, open('results/bilinear/model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\debor\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "Epoch 1000 | mean loss: 5.29200: 100%|██████████| 1000/1000 [01:57<00:00,  8.50epoch/s]\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# Model Training ConvKB\n",
    "############################################################\n",
    "#KGs = pickle.load(open('data/all_Knowledge_Graphs.pkl', 'rb'))\n",
    "# Load dataset\n",
    "#kg_train, _, _ = load_fb15k()\n",
    "#kg_train = KGs[:10]\n",
    "#kg_val = KGs[103]\n",
    "#kg_test = KGs[200]\n",
    "\n",
    "# Define some hyper-parameters for training\n",
    "emb_dim = 10\n",
    "lr = 0.0004\n",
    "n_epochs = 1000\n",
    "b_size = 128\n",
    "margin = 0.5\n",
    "\n",
    "kg_train = complete_KG\n",
    "\n",
    "# Define the model and criterion\n",
    "model = ConvKBModel(emb_dim, 12, kg_train.n_ent, kg_train.n_rel)\n",
    "criterion = MarginLoss(margin)\n",
    "\n",
    "# Move everything to CUDA if available\n",
    "if cuda.is_available():\n",
    "    cuda.empty_cache()\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "\n",
    "# Define the torch optimizer to be used\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "sampler = BernoulliNegativeSampler(kg_train) #UniformNegativeSampler(kg_train, kg_val=kg_val, kg_test=kg_test)\n",
    "dataloader = DataLoader(kg_train, batch_size=b_size)\n",
    "\n",
    "iterator = tqdm(range(n_epochs), unit='epoch')\n",
    "for epoch in iterator:\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        h, t, r = batch[0], batch[1], batch[2]\n",
    "        n_h, n_t = sampler.corrupt_batch(h, t, r)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        pos, neg = model(h, t, r, n_h, n_t)\n",
    "        loss = criterion(pos, neg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    iterator.set_description(\n",
    "        'Epoch {} | mean loss: {:.5f}'.format(epoch + 1,\n",
    "                                              running_loss / len(dataloader)))\n",
    "    model.normalize_parameters()\n",
    "model.normalize_parameters()\n",
    "\n",
    "pickle.dump(model, open('results/deep/model.pkl', 'wb')) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7927a50cdc078fee6c0386384f02243f9c31fc413ad7647fe6d6c063eb6496c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
